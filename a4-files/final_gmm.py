# -*- coding: utf-8 -*-
"""Final GMM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IiXEhNS9bWDV4Pw2VvdzRL1R_6D7B0_p
"""

import numpy as np
import pandas as pd
#from google.colab import drive

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import time
from tqdm import tqdm
from scipy.special import logsumexp
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

#drive.mount('/content/gdrive')

data = pd.read_csv('./data/gmm_dataset.csv', header = None).values
"""## MNIST"""

class GMM:
  def __init__(self, k, max_iter, tol = 10e-5):
    self.k = k
    self.max_iter = max_iter
    self.tol = tol
    self.loss = np.array([])
    self.S = None
    self.mu = None
    self.r = np.zeros(self.k)
    pi = np.random.random(self.k)
    self.pi = pi/np.sum(pi)
  
  def fit(self, x):
    n_data, dim_data = x.shape
    self.S = 1*np.ones((self.k, dim_data))
    self.mu = np.max(x, axis = 0) * np.random.random((self.k, dim_data))
    log_r_ik = np.zeros((self.k, n_data))
    for iteration in range(self.max_iter):
      for k in range(self.k):
        numerator = (x - self.mu[k])**2
        index = np.sum(np.divide(numerator, self.S[k], out = np.zeros_like(numerator), where = self.S[k] != 0), 1)
        index = index - np.min(index)
        first = np.sum(np.log(2*np.pi*self.S[k], out = np.zeros_like(self.S[k]), where = self.S[k] != 0))
        log_r_ik[k] = np.log(self.pi[k], out = np.zeros_like(self.pi[k]), where = self.pi[k] != 0) - 0.5 *first -0.5*index
      
      r_ik = np.exp(log_r_ik)
      r_i = np.sum(r_ik, axis = 0)
      self.loss = np.append(self.loss, - np.sum(np.log(r_i, out = np.zeros_like(r_i), where = r_i!=0)))
      log_r_ik = (log_r_ik - np.min(log_r_ik, axis=0))
      r_ik = np.exp(log_r_ik)
      r_i = np.sum(r_ik, axis = 0)
      r_ik = r_ik/r_i
      
      if iteration > 1 and np.abs(self.loss[iteration] - self.loss[iteration - 1]) <= self.tol * np.abs(self.loss[iteration]):
        break
      for k in range(self.k):
        self.r[k] = np.sum(r_ik[k])
        self.pi[k] = self.r[k]/n_data
        un_norm_mu = np.matmul(r_ik[k], x)
        self.mu[k] = np.divide(un_norm_mu, self.r[k], out =np.zeros_like(un_norm_mu), where =  self.r[k] != 0)
        un_norm_s = np.sum(np.expand_dims(r_ik, axis = 0).T * (x - self.mu[k]) ** 2,0)
        self.S[k] = np.divide(un_norm_s, self.r[k], out = np.zeros_like(un_norm_s), where = self.r[k] != 0)
      return self.loss

  def predict(self, x):
    n_data, dim_data = x.shape 
    log_r_ik = np.zeros((self.k, n_data))
    for k in range(self.k):
      numerator = (x - self.mu[k])**2
      index = np.sum(np.divide(numerator, self.S[k], out = np.zeros_like(numerator), where= self.S[k] != 0), 1)
      index = index - np.min(index)
      first = np.sum(np.log(2*np.pi*self.S[k], out = np.zeros_like(self.S[k]), where = self.S[k] != 0))
      log_r_ik[k] = np.log(self.pi[k], out = np.zeros_like(self.pi[k]), where = self.pi[k] != 0) - 0.5 *first -0.5*index
    return log_r_ik
"""
from keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.astype('float32')/255
x_test = x_test.astype('float32')/255

x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))
"""
print("start loading")
##################################
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Transform to normalized Tensors
transform = transforms.Compose([transforms.ToTensor()])

train_dataset = datasets.MNIST('./data', train=True, transform=transform, download=True)
test_dataset = datasets.MNIST('./data', train=False, transform=transform, download=True)

train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))
test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))

mnist_training_x = next(iter(train_loader))[0].numpy()
mnist_test_x = next(iter(test_loader))[0].numpy()

x_train = 255*mnist_training_x.reshape([mnist_training_x.shape[0], mnist_training_x.shape[2]*mnist_training_x.shape[3]])
x_test = 255*mnist_test_x.reshape([mnist_test_x.shape[0], mnist_test_x.shape[2]*mnist_test_x.shape[3]])
y_train = next(iter(train_loader))[1].numpy()
y_test = next(iter(test_loader))[1].numpy()
##################################
print("done")


pca = PCA()
# PCA for dimensionality redcution (non-visualization)
pca.n_components = 784
pca_data = pca.fit_transform(x_train)
percentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_);
cum_var_explained = np.cumsum(percentage_var_explained)

plt.figure(1, figsize=(6, 4))
plt.clf()
plt.plot(cum_var_explained, linewidth=2)
plt.axis('tight')
plt.grid()
plt.xlabel('n_components')
plt.ylabel('Cumulative_explained_variance')
plt.show()
#We can use 300 components

pca = PCA()
# PCA for dimensionality redcution (non-visualization)
pca.n_components = 300
pca_train_data = pca.fit_transform(x_train)
pca_test_data = pca.transform(x_test)

x_train[np.where(y_train == 5)]

for num in tqdm(range(10)):
  model = GMM(k = k, max_iter= 500)
  loss = model.fit(pca_train_data[np.where(y_train == num)])

class_prob = []
log_prob = []

for num in range(10):
  class_prob.append(len(y_train[y_train == num])/len(pca_test_data))
  class_prob = np.array(class_prob)

log_prob = model.predict(pca_test_data)
log_prob = log_prob - np.min(np.max(log_prob, axis = 0), axis = 0)

prob = (logsumexp(log_prob.T + np.log(class_prob), axis = 1)).T
max_prob = np.argmax(prob, axis = 0)

accuracy = len(max_prob[max_prob == y_test])/len(max_prob)


accuracy = np.array(accuracy)
plt.plot(k_values, 1 - accuracy)
plt.xlabel('k')
plt.ylabel('Error Rate')
plt.title('Error rate with different K on MNIST Data')
plt.show()

